{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aim: Ad-hoc classifier of Python 2 vs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC # not stochastic\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "import warnings\n",
    "#import requests\n",
    "#from bs4 import BeautifulSoup\n",
    "#import time\n",
    "#from fake_useragent import UserAgent\n",
    "#import sys, os\n",
    "import flask\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "TEST_SIZE = 0.3\n",
    "MYFOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CSS = \"\"\"\n",
    "body {\n",
    "    margin: 0;\n",
    "    font-family: Helvetica;\n",
    "}\n",
    "table.dataframe {\n",
    "    border-collapse: collapse;\n",
    "    border: none;\n",
    "}\n",
    "table.dataframe tr {\n",
    "    border: none;\n",
    "}\n",
    "table.dataframe td, table.dataframe th {\n",
    "    margin: 0;\n",
    "    border: 1px solid white;\n",
    "    padding-left: 0.25em;\n",
    "    padding-right: 0.25em;\n",
    "}\n",
    "table.dataframe th:not(:empty) {\n",
    "    background-color: #fff5cc;\n",
    "    text-align: left;\n",
    "    font-size: 18px;\n",
    "    font-family: \"Baloo\"\n",
    "}\n",
    "table.dataframe tr:nth-child(2) th:empty {\n",
    "    border-left: none;\n",
    "    border-right: 1px dashed #888;\n",
    "}\n",
    "table.dataframe td {\n",
    "    border: 2px solid #0e65f1;\n",
    "    background-color: #e7f0fe;\n",
    "    font-size: 16px;\n",
    "    font-family: \"Gloria Hallelujah\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML('<link href=\"https://fonts.googleapis.com/css?family=Baloo|Gloria+Hallelujah\" rel=\"stylesheet\"><style>{}</style>'.format(CSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = pd.DataFrame(rf_grp.feature_importances_, columns = [\"imp\"])\n",
    "imp['Feature Name'] = X_cols_label\n",
    "imp.sort_values('imp', ascending = False, inplace = True)\n",
    "imp.reset_index(inplace=True, drop = True)\n",
    "\n",
    "#make stuff pretty\n",
    "imp['Importancy Ranking'] = pd.Series(np.array(range(len(X_cols_label))) + 1).map(str)\n",
    "del imp['imp']\n",
    "imp.loc[imp['Importancy Ranking'] == \"37\", :] = [\"..\", \"..\"]\n",
    "imp.sort_index(inplace = True)\n",
    "imp = imp[['Importancy Ranking', 'Feature Name']].iloc[[0, 1, 2, 3, 4, 36, 37, 38, 39, 40, 41], :]\n",
    "#imp.columns = pd.MultiIndex.from_arrays([[\"Importancy\", 'Feature'], ['Ranking','Name']], names=['',''])\n",
    "imp.index = [\"\"] * len(imp)\n",
    "#imp.rename(index = str, columns = {'Importancy Ranking': \"Importancy\\nRanking\"}, inplace = True)\n",
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"cleaned2_data.pkl\", 'rb') as picklefile: \n",
    "    df_all = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_cols = [col for col in df_all.columns if col not in ['q_id', 'q_user', 'a_id', 'a_user', 'label_col']]\n",
    "X_cols_label = ['Question Score', 'Question Views', 'Answer Score', 'Asker Reputation', 'Asker Gold Badges', \n",
    "    'Asker Silver Badges', 'Asker Bronze Badges', 'Answerer Reputation', 'Answerer Gold Badges', \n",
    "    'Answerer Silver Badges', 'Answerer Bronze Badges', 'Array Tag',\n",
    "    'Beautifulsoup Tag', 'CSV Tag', 'Dataframe Tag', 'Datetime Tag', 'Dictionary Tag', 'Django Tag',\n",
    "    'Flask Tag', 'Json Tag', 'List Tag', 'Matplotlib Tag', 'Numpy Tag', 'Pandas Tag', 'Python Tag',\n",
    "    'Regex Tag', 'String Tag', 'Unicode Tag', 'Unit-testing Tag', 'Log Seconds between Question and Answer',\n",
    "    'Year of Question', 'Month of Question', 'Day of Question', 'Day of Week of Question', \n",
    "    'Log Seconds between Question and Present Day', 'Year of Answer',\n",
    "    'Month of Answer', 'Day of Answer', 'Day of Week of Answer', 'Log Seconds between Answer and Present Day', \n",
    "    'Length of Question Title', 'Length of Question', 'Length of Answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample questions, not rows, b/c one person esp. power users can ask multiple questions \n",
    "id_set = df_all[['q_id']].drop_duplicates()\n",
    "ids, id_holdout = train_test_split(id_set, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10043, 48) (10043, 43) (10043,) (4343, 48) (4343, 43) (4343,) 2.3124568270780568\n"
     ]
    }
   ],
   "source": [
    "df = df_all.loc[df_all['q_id'].map(lambda x: x in ids['q_id'].tolist())]\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df_holdout = df_all.loc[df_all['q_id'].map(lambda x: x in id_holdout['q_id'].tolist())]\n",
    "df_holdout.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X = df[X_cols]\n",
    "y = df['label_col']\n",
    "\n",
    "X_holdout = df_holdout[X_cols]\n",
    "y_holdout = df_holdout['label_col']\n",
    "print(df.shape, X.shape, y.shape, df_holdout.shape, X_holdout.shape, y_holdout.shape, \n",
    "      y.shape[0] / y_holdout.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a tr/ts for plotting ROC curve\n",
    "trid_for_roc, tsid_for_roc = train_test_split(ids, test_size = TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_for_roc = df_all.loc[df_all['q_id'].map(lambda x: x in trid_for_roc['q_id'].tolist())]\n",
    "tr_for_roc.reset_index(drop = True, inplace = True)\n",
    "ts_for_roc = df_all.loc[df_all['q_id'].map(lambda x: x in tsid_for_roc['q_id'].tolist())]\n",
    "ts_for_roc.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_tr_for_roc = tr_for_roc[X_cols]\n",
    "y_tr_for_roc = tr_for_roc['label_col']\n",
    "\n",
    "X_ts_for_roc = ts_for_roc[X_cols]\n",
    "y_ts_for_roc = ts_for_roc['label_col']\n",
    "# print(tr_for_roc.shape, X_tr_for_roc.shape, y_tr_for_roc.shape, ts_for_roc.shape, \n",
    "#       X_ts_for_roc.shape, y_ts_for_roc.shape, y_tr_for_roc.shape[0] / y_ts_for_roc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_metrics_cv5 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_cols = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"model\"]\n",
    "def get_metrics(model, X = X, cv = MYFOLDS):\n",
    "    return [np.mean(cross_val_score(model, X, y, groups = df['q_id'], cv = cv, scoring = make_scorer(accuracy_score))),\n",
    "          np.mean(cross_val_score(model, X, y, groups = df['q_id'], cv = cv, scoring = make_scorer(precision_score))),\n",
    "          np.mean(cross_val_score(model, X, y, groups = df['q_id'], cv = cv, scoring = make_scorer(recall_score))),\n",
    "          np.mean(cross_val_score(model, X, y, groups = df['q_id'], cv = cv, scoring = make_scorer(f1_score)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(nb) + [\"GaussianNB\"]).reshape(1, 5)))\n",
    "model_metrics_cv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stochastic linaer SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss = 'hinge', penalty = 'l1', alpha = 0.0001, random_state = RANDOM_STATE)\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(svm) + [\"SVM\"]).reshape(1, 5)))\n",
    "model_metrics_cv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stochastic SVM with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbf_X_fun = RBFSampler(gamma = 1, random_state = RANDOM_STATE)\n",
    "rbf_X_fun.fit(X)\n",
    "rbf_X = rbf_X_fun.transform(X)\n",
    "\n",
    "rbf_svm = SGDClassifier(loss = 'hinge', penalty = 'l1', alpha = 0.0001, random_state = RANDOM_STATE)\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(rbf_svm, X = rbf_X) + [r\"SVM(RBF)\"]).reshape(1, 5)))\n",
    "model_metrics_cv5\n",
    "#TODO GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- logistic reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(logr) + [\"LogisticReg\"]).reshape(1, 5)))\n",
    "model_metrics_cv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logistic reg w LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logr_lasso = LogisticRegression(penalty = 'l1', C = 1)\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(logr_lasso) + [r\"LogisticReg(LASSO)\"]).reshape(1, 5)))\n",
    "model_metrics_cv5\n",
    "#TODO GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(knn) + [\"KNN(5)\"]).reshape(1, 5)))\n",
    "model_metrics_cv5\n",
    "#TODO loop thru diff k's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gboost = GradientBoostingClassifier()\n",
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(gboost) + [\"GBoost\"]).reshape(1, 5)))\n",
    "model_metrics_cv5\n",
    "#TODO GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mygrid = {\"n_estimators\": [10, 20, 30, 40], \n",
    "          \"max_depth\": [5, 7, 9, 11, 13],\n",
    "          \"min_samples_split\": [2, 0.02],\n",
    "          \"min_samples_leaf\": [1, 0.01]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid = mygrid, scoring = make_scorer(f1_score), cv = MYFOLDS)\n",
    "fit = grid_search.fit(X, y, groups = df['q_id'])\n",
    "all_result = grid_search.cv_results_\n",
    "rf_final = grid_search.best_estimator_\n",
    "print(rf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_metrics_cv5 = model_metrics_cv5.append(pd.DataFrame(np.array(get_metrics(rf_final) + [\"RF\"]).reshape(1, 5)))\n",
    "model_metrics_cv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_metrics_cv5.columns = metric_cols\n",
    "model_metrics_cv5.reset_index(inplace = True, drop = True)\n",
    "model_metrics_cv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 7))\n",
    "plt.plot(model_metrics_cv5.index, model_metrics_cv5['Accuracy'], \"b-o\", label = 'Accuracy', linewidth = 2)\n",
    "plt.plot(model_metrics_cv5.index, model_metrics_cv5['Precision'], \"y-o\", label = 'Precision', linewidth = 2)\n",
    "plt.plot(model_metrics_cv5.index, model_metrics_cv5['Recall'], \"g-o\", label = 'Recall', linewidth = 2)\n",
    "plt.plot(model_metrics_cv5.index, model_metrics_cv5['F1'], \"r-o\", label = 'F1', linewidth = 2)\n",
    "plt.xticks(model_metrics_cv5.index, model_metrics_cv5['model'], fontsize = 16, fontweight = 'bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.yticks(np.array(range(0, 101, 10)) / 100, fontsize = 16)\n",
    "plt.legend(loc = 'lower right', fontsize = 16)\n",
    "plt.title(r\"Model Metric Comparison (5 Fold CV)\", fontsize = 22, fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_final.fit(X_tr_for_roc, y_tr_for_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_metrics2(X, y, whatdata, final_model = rf_final, mypos_label = 1): #label_col == 1 means it's a Python3 question\n",
    "    pred = final_model.predict(X)\n",
    "    accuracy = accuracy_score(y, pred)\n",
    "    precision = precision_score(y, pred, pos_label = mypos_label) \n",
    "    recall = recall_score(y, pred, pos_label = mypos_label)\n",
    "    f1 = f1_score(y, pred, pos_label = mypos_label)\n",
    "    \n",
    "    pred_proba = final_model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thres = roc_curve(y, pred_proba, pos_label = mypos_label)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "\n",
    "    mystr = \"Random Forest \" + whatdata + \\\n",
    "      \" accuracy \" +  str(format(accuracy, \"8.4f\")).strip(\" \") + \\\n",
    "      \" precision \" +  str(format(precision, \"8.4f\")).strip(\" \") + \\\n",
    "      \" recall \" +  str(format(recall, \"8.4f\")).strip(\" \") + \\\n",
    "      \" f1 \" +  str(format(f1, \"8.4f\")).strip(\" \") + \\\n",
    "      \" auc \" +  str(format(auc_value, \"8.4f\")).strip(\" \")\n",
    "    \n",
    "    return pred, pred_proba, accuracy, precision, recall, f1, fpr, tpr, thres, auc_value, mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_ts, pred_proba_ts, accuracy_ts, precision_ts, recall_ts, \\\n",
    "f1_ts, fpr_ts, tpr_ts, thres_ts, auc_value_ts, mystr_ts = get_metrics2(X_ts_for_roc, y_ts_for_roc, \"testing set\")\n",
    "print(mystr_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred, pred_proba, accuracy, precision, recall, \\\n",
    "f1, fpr, tpr, thres, auc_value, mystr = get_metrics2(X_holdout, y_holdout, \"holdout set\")\n",
    "print(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cond = model_metrics_cv5['model'] == \"RF\"\n",
    "mystr_cv = \"Random Forest \" + \"5 fold CV\" + \\\n",
    "      \" accuracy \" +  str(format(float(model_metrics_cv5.loc[cond, \"Accuracy\"]), \"8.4f\")).strip(\" \") + \\\n",
    "      \" precision \" +  str(format(float(model_metrics_cv5.loc[cond, \"Precision\"]), \"8.4f\")).strip(\" \") + \\\n",
    "      \" recall \" +  str(format(float(model_metrics_cv5.loc[cond, \"Recall\"]), \"8.4f\")).strip(\" \") + \\\n",
    "      \" f1 \" +  str(format(float(model_metrics_cv5.loc[cond, \"F1\"]), \"8.4f\")).strip(\" \") + \\\n",
    "      \" auc -.----    \"\n",
    "print(mystr_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 4))\n",
    "plt.plot(fpr_ts, tpr_ts, \"b-\", label = \"Testing\")\n",
    "plt.plot(fpr, tpr, \"r-\", label = \"Holdout\")\n",
    "plt.suptitle(\"Random Forest Classifier ROC Curve\", fontsize = 18, fontweight = 'bold')\n",
    "plt.title(\"\", fontsize = 4)\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize = 14, fontweight = 'bold')\n",
    "plt.annotate(mystr_cv + \"\\n\" + mystr_ts + \"\\n\" + mystr + \"\\n\\n\\n\", xy=(0.935, 0), xycoords='figure fraction',\n",
    "            xytext=(0, -45), textcoords='offset points',\n",
    "            ha = \"right\", fontsize = 14, style='italic')\n",
    "plt.yticks(fontweight = 'bold')\n",
    "plt.xticks(fontweight = 'bold')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = pd.DataFrame(rf_final.feature_importances_, columns = [\"imp\"])\n",
    "imp['feat'] = X_cols_label\n",
    "imp.sort_values('imp', ascending = False, inplace = True)\n",
    "imp.set_index(np.array(range(len(X_cols_label)))+1, inplace = True)\n",
    "imp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(pd.DataFrame(np.array(y_holdout), columns = [\"True\"]).iloc[:, 0], \\\n",
    "            pd.DataFrame(np.array(pred), columns = [\"Pred\"]).iloc[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- grouped/ungrouped train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix a holdout set (correctly grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_set = df_all[['q_id']].drop_duplicates()\n",
    "ids, id_holdout = train_test_split(id_set, test_size = TEST_SIZE, random_state = 6745)\n",
    "df = df_all.loc[df_all['q_id'].map(lambda x: x in ids['q_id'].tolist())]\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df_holdout = df_all.loc[df_all['q_id'].map(lambda x: x in id_holdout['q_id'].tolist())]\n",
    "df_holdout.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X = df[X_cols]\n",
    "y = df['label_col']\n",
    "\n",
    "X_holdout = df_holdout[X_cols]\n",
    "y_holdout = df_holdout['label_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tr w/o grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size = TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_ungrp = RandomForestClassifier()\n",
    "rf_ungrp.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_pred = rf_ungrp.predict(X_tr)\n",
    "acc = accuracy_score(y_tr, tr_pred)\n",
    "precision = precision_score(y_tr, tr_pred)\n",
    "recall = recall_score(y_tr, tr_pred)\n",
    "f1 = f1_score(y_tr, tr_pred)\n",
    "print(\"tr ungrp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_pred = rf_ungrp.predict(X_ts)\n",
    "acc = accuracy_score(y_ts, ts_pred)\n",
    "precision = precision_score(y_ts, ts_pred)\n",
    "recall = recall_score(y_ts, ts_pred)\n",
    "f1 = f1_score(y_ts, ts_pred)\n",
    "print(\"ts ungrp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hold_pred = rf_ungrp.predict(X_holdout)\n",
    "acc = accuracy_score(y_holdout, hold_pred)\n",
    "precision = precision_score(y_holdout, hold_pred)\n",
    "recall = recall_score(y_holdout, hold_pred)\n",
    "f1 = f1_score(y_holdout, hold_pred)\n",
    "print(\"hold ungrp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tr w grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_tr, id_ts = train_test_split(ids, test_size = TEST_SIZE, random_state = 6745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = df_all.loc[df_all['q_id'].map(lambda x: x in id_tr['q_id'].tolist())]\n",
    "ts = df_all.loc[df_all['q_id'].map(lambda x: x in id_ts['q_id'].tolist())]\n",
    "X_tr = tr[X_cols]\n",
    "y_tr = tr['label_col']\n",
    "\n",
    "X_ts = ts[X_cols]\n",
    "y_ts = ts['label_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_grp = RandomForestClassifier()\n",
    "rf_grp.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_pred = rf_grp.predict(X_tr)\n",
    "acc = accuracy_score(y_tr, tr_pred)\n",
    "precision = precision_score(y_tr, tr_pred)\n",
    "recall = recall_score(y_tr, tr_pred)\n",
    "f1 = f1_score(y_tr, tr_pred)\n",
    "print(\"tr grp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_pred = rf_grp.predict(X_ts)\n",
    "acc = accuracy_score(y_ts, ts_pred)\n",
    "precision = precision_score(y_ts, ts_pred)\n",
    "recall = recall_score(y_ts, ts_pred)\n",
    "f1 = f1_score(y_ts, ts_pred)\n",
    "print(\"ts grp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hold_pred = rf_grp.predict(X_holdout)\n",
    "acc = accuracy_score(y_holdout, hold_pred)\n",
    "precision = precision_score(y_holdout, hold_pred)\n",
    "recall = recall_score(y_holdout, hold_pred)\n",
    "f1 = f1_score(y_holdout, hold_pred)\n",
    "print(\"hold grp\", acc, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize = (16, 4))\n",
    "plt.suptitle(\"Two Different Ways to Split Train/Testing Data\\n\", fontsize = 18, fontweight = 'bold', y = 1.03)\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1, 2, 3], [0.999715868731, 1.0, 0.999404229967, 0.999702026222], 'b-o', label = \"Training\")\n",
    "plt.plot([0, 1, 2, 3], [0.977461054027, 0.98392732355, 0.969029593944, 0.976421636616], 'r-o', label = \"Testing\")\n",
    "plt.plot([0, 1, 2, 3], [0.639030023095, 0.649519890261, 0.473736868434, 0.547873879086], 'y-o', label = \"Holdout\")\n",
    "plt.xticks([0, 1, 2, 3], [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"], fontsize = 14, fontweight = 'bold')\n",
    "plt.yticks(fontsize = 14, fontweight = 'bold')\n",
    "plt.legend(fontsize = 14)\n",
    "plt.title(\"Ungrouped\", fontsize = 14, fontweight = 'bold', style = 'italic')\n",
    "ax2 = plt.subplot(1, 2, 2, sharey = ax1)\n",
    "plt.plot([0, 1, 2, 3], [1, 1, 1, 1], 'b-o', label = \"Training\")\n",
    "plt.plot([0, 1, 2, 3], [0.645608108108, 0.640766902119, 0.478162650602, 0.547649849073], 'r-o', label = \"Testing\")\n",
    "plt.plot([0, 1, 2, 3], [0.600230946882, 0.587012987013, 0.452226113057, 0.510878779316], 'y-o', label = \"Holdout\")\n",
    "ax2.set_ylim((0, 1.1))\n",
    "plt.xticks([0, 1, 2, 3], [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"], fontsize = 14, fontweight = 'bold')\n",
    "plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.title(\"Group by Question ID\", fontsize = 14, fontweight = 'bold', style = 'italic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search_ungrp = GridSearchCV(RandomForestClassifier(), param_grid = mygrid, scoring = make_scorer(f1_score), cv = MYFOLDS)\n",
    "# grid_search_ungrp.fit(X, y)\n",
    "# rf_ungrp = grid_search_ungrp.best_estimator_\n",
    "# print(rf_ungrp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.mean(cross_val_score(rf_ungrp, X, y, cv = MYFOLDS, scoring = make_scorer(accuracy_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, cv = MYFOLDS, scoring = make_scorer(precision_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, cv = MYFOLDS, scoring = make_scorer(recall_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, cv = MYFOLDS, scoring = make_scorer(f1_score)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[np.mean(cross_val_score(rf_ungrp, X, y, groups = df['q_user'], cv = MYFOLDS, scoring = make_scorer(accuracy_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, groups = df['q_user'], cv = MYFOLDS, scoring = make_scorer(precision_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, groups = df['q_user'], cv = MYFOLDS, scoring = make_scorer(recall_score))), \\\n",
    "np.mean(cross_val_score(rf_ungrp, X, y, groups = df['q_user'], cv = MYFOLDS, scoring = make_scorer(f1_score)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tr_ungrp, X_ts_ungrp, y_tr_ungrp, y_ts_ungrp = train_test_split(X, y, test_size = TEST_SIZE)\n",
    "rf_ungrp.fit(X_tr_ungrp, y_tr_ungrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trid_for_roc, tsid_for_roc = train_test_split(ids, test_size = TEST_SIZE)\n",
    "tr_for_roc = df_all.loc[df['q_user'].map(lambda x: x in trid_for_roc['q_user'].tolist())]\n",
    "tr_for_roc.reset_index(drop = True, inplace = True)\n",
    "ts_for_roc = df_all.loc[df['q_user'].map(lambda x: x in tsid_for_roc['q_user'].tolist())]\n",
    "ts_for_roc.reset_index(drop = True, inplace = True)\n",
    "\n",
    "X_tr_for_roc = tr_for_roc[X_cols]\n",
    "y_tr_for_roc = tr_for_roc['label_col']\n",
    "\n",
    "X_ts_for_roc = ts_for_roc[X_cols]\n",
    "y_ts_for_roc = ts_for_roc['label_col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_ungrp = rf_ungrp.predict(X_holdout)\n",
    "accuracy_ungrp = accuracy_score(y_holdout, pred_ungrp)\n",
    "precision_ungrp = precision_score(y_holdout, pred_ungrp, pos_label = 1) \n",
    "recall_ungrp = recall_score(y_holdout, pred_ungrp, pos_label = 1)\n",
    "f1_ungrp = f1_score(y_holdout, pred_ungrp, pos_label = 1)\n",
    "print(accuracy_ungrp, precision_ungrp, recall_ungrp, f1_ungrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_tree = rf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.columns[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_f_imp = pd.DataFrame(rf.feature_importances_, columns = [\"imp\"])\n",
    "avg_f_imp['name'] = X.columns\n",
    "avg_f_imp.sort_values('imp', ascending = False, inplace = True)\n",
    "#avg_f_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "export_graphviz(first_tree)\n",
    "first_tree.tree_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_tree.tree_.children_left[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_tree.tree_.children_right[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all[\"label_col\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### pickle model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- rf for viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(rf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest  accuracy 0.6016 precision 0.7029 recall 0.3394 f1 0.4578 auc 0.6464\n"
     ]
    }
   ],
   "source": [
    "rf_viz = RandomForestClassifier(n_estimators=40, max_depth=3)\n",
    "rf_viz.fit(X_tr_for_roc, y_tr_for_roc)\n",
    "pred_viz, pred_proba_viz, accuracy_viz, precision_viz, recall_viz, \\\n",
    "f1_viz, fpr_viz, tpr_viz, thres_viz, auc_value_viz, mystr_viz = \\\n",
    "get_metrics2(X_ts_for_roc, y_ts_for_roc, \"\", final_model=rf_viz)\n",
    "print(mystr_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_for_viz.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(rf_viz, picklefile)\n",
    "    pickle.dump(X_cols_label, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
