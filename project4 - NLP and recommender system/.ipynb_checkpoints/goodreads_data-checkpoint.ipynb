{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/home/katharina/anaconda3/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If not running on AWS instance,\n",
    "# keep this running in the command line:\n",
    "# ssh -NL 12345:localhost:27017 myaws &\n",
    "# To kill:\n",
    "# use ps | grep \"ssh -NL\" to find jobID, then\n",
    "# kill [jobID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "settings port_num 27017 st_page 61 end_page 70 del_all_result False\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "st_page, end_page = 1, 10\n",
    "port_num = 27017\n",
    "del_all_result = False\n",
    "genre = r\"alternate-history\"\n",
    "api_key = \"fqX09wQYnt4aDOJxQoRtkQ\"\n",
    "delay = 5\n",
    "ua = UserAgent()\n",
    "print(\"settings\", \"port_num\", port_num, \"st_page\", st_page, \"end_page\", end_page, \"del_all_result\", del_all_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bookids(st_page, end_page):\n",
    "    try:     \n",
    "        book_li = []\n",
    "        for current_page in range(st_page, end_page + 1):      \n",
    "            user_agent = {'User-agent': ua.random}\n",
    "            url = r\"https://www.goodreads.com/shelf/show/\" + genre + r\"?page=\" + str(current_page)\n",
    "            page = requests.get(url, headers = user_agent)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "            book_links = soup.find_all(\"a\", {\"class\", \"bookTitle\"})\n",
    "            for link in book_links:\n",
    "                bookid = re.sub(\"\\/book\\/show\\/(?P<bookid>[0-9]+).*\", \"\\g<bookid>\", link[\"href\"])\n",
    "                book_li += [bookid]\n",
    "            time.sleep(10 * np.random.rand())\n",
    "        return book_li\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrape_to_mongo(bookid):\n",
    "    try:\n",
    "        # scrape one book\n",
    "        user_agent = {'User-agent': ua.random}\n",
    "        url = r\"https://www.goodreads.com/book/show.xml?key=\" + api_key + r\"&id=\" + str(bookid)\n",
    "        page = requests.get(url, headers = user_agent)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "        one_book_desc = soup.find(\"description\").text\n",
    "        one_title = soup.find(\"h1\").text\n",
    "        one_author = soup.find(\"name\").text\n",
    "        one_page_ct = soup.find_all(\"num_pages\")[1].text\n",
    "        one_pub_yr = soup.find(\"publication_year\").text\n",
    "        one_lang = soup.find(\"language_code\").text\n",
    "        n_text_reviews = soup.find(\"text_reviews_count\").text\n",
    "        n_ratings = soup.find(\"ratings_count\").text\n",
    "        avg_rating = soup.find(\"average_rating\").text\n",
    "\n",
    "        bookColl.insert_one({\"bookid\": bookid, \n",
    "                             \"title\": one_title,\n",
    "                             \"author\": one_author,\n",
    "                             \"page_ct\": one_page_ct,\n",
    "                             \"pub_yr\": one_pub_yr,\n",
    "                             \"leng\": one_lang,\n",
    "                             \"n_text_reviews\": n_text_reviews,\n",
    "                             \"n_ratings\": n_ratings,\n",
    "                             \"avg_rating\": avg_rating,\n",
    "                             \"desc\": one_book_desc\n",
    "                            })\n",
    "\n",
    "        iframe = soup.find(\"iframe\")\n",
    "        url = re.sub(r\"DEVELOPER_ID\", api_key, iframe['src']) # url to first page of reviews\n",
    "        page = requests.get(url, headers = user_agent)\n",
    "        soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "        # get links to review pages\n",
    "        all_review_links = []\n",
    "        all_review_links.extend(soup.find_all(\"link\"))\n",
    "        some_next_pages = soup.find(\"div\", {\"class\": \"gr_pagination\"}).find_all(\"a\") \n",
    "                # urls to next pages of reviews\n",
    "                # Note: the num of pages are capped around 9\n",
    "        for review_page in some_next_pages:\n",
    "            part_url = review_page[\"href\"]\n",
    "            url = r\"https://www.goodreads.com\" + part_url\n",
    "            page = requests.get(url, headers = user_agent)\n",
    "            soup = BeautifulSoup(page.text, 'lxml')  \n",
    "            all_review_links.extend(soup.find_all(\"link\"))\n",
    "\n",
    "        # get reviews\n",
    "        for link in all_review_links: \n",
    "            url = link[\"href\"]\n",
    "            if re.match(\"https:\\/\\/www\\.goodreads\\.com\\/review\\/show\\/.*\", url):\n",
    "                page = requests.get(url, headers = user_agent)\n",
    "                soup = BeautifulSoup(page.text, 'lxml')\n",
    "                user_obj = soup.find(\"span\", {\"class\": \"reviewer\"})\n",
    "                one_userid = re.sub(r\"\\/user\\/show\\/(?P<userid>[0-9]+)\\-.*\", r\"\\g<userid>\", user_obj.find(\"a\")[\"href\"])\n",
    "                one_review_yr = soup.find(\"span\", {\"itemprop\": \"publishDate\"}).text\n",
    "                one_review = soup.find(\"div\", {\"class\", \"reviewText\"}).text\n",
    "                one_like_ct = re.sub(\"[^0-9]*\", \"\", soup.find(\"span\", {\"class\": \"likesCount\"}).text)\n",
    "                one_rating = soup.find(\"div\", {\"class\", \"rating\"}).find(\"span\", {\"class\", \"value-title\"})[\"title\"]\n",
    "\n",
    "                reviewColl.insert_one({\"bookid\": bookid, \n",
    "                                       \"userid\": one_userid,\n",
    "                                       \"rating\": one_rating, \n",
    "                                       \"review_yr\": one_review_yr,\n",
    "                                       \"like_ct\": one_like_ct,\n",
    "                                       \"review\": one_review})\n",
    "        time.sleep(delay + 2 * np.random.rand())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mongoClient = MongoClient(port = port_num)\n",
    "bookdb = mongoClient.bookdb\n",
    "bookColl = bookdb.bookColl\n",
    "reviewdb = mongoClient.reviewdb\n",
    "reviewColl = reviewdb.reviewColl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if del_all_result:\n",
    "    del_result = bookColl.delete_many({})\n",
    "    del_result = reviewColl.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got all bookids\n"
     ]
    }
   ],
   "source": [
    "book_li = get_bookids(st_page, end_page)\n",
    "print(\"Got all bookids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted to mongo\n"
     ]
    }
   ],
   "source": [
    "for bookid in book_li:\n",
    "    scrape_to_mongo(bookid)\n",
    "print(\"Inserted to mongo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
